{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "demo_monolstm_flickr8k.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Subangkar/Image-Captioning-Attention-PyTorch/blob/main/demo_monolstm_flickr8k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXcShO2s1lrs"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VresdQRG1wlO"
      },
      "source": [
        "###Fetch Dataset & Codes from GitHub\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71DeHINu_VmP",
        "outputId": "2bd132b7-48d7-48a2-d885-945a819ad861"
      },
      "source": [
        "%%shell\r\n",
        "sudo apt-get install git-lfs"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (2.3.4-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66MgXxG526O-"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "if [ ! -f \"train_attntn.py\" ]\r\n",
        "then\r\n",
        "    git clone https://github.com/Subangkar/Image-Captioning-Attention-PyTorch/\r\n",
        "    pushd Image-Captioning-Attention-PyTorch/ && git lfs install && git lfs fetch --all && popd\r\n",
        "    mv Image-Captioning-Attention-PyTorch/* .\r\n",
        "    rm -r Image-Captioning-Attention-PyTorch\r\n",
        "fi\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3QLq56f1zV0"
      },
      "source": [
        "# %%capture\r\n",
        "%%shell\r\n",
        "mkdir -p data/flickr8k/\r\n",
        "if [ ! -f \"data/flickr8k/Flickr8k_Dataset.zip\" ]\r\n",
        "then\r\n",
        "    wget \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\" -O \"data/flickr8k/Flickr8k_Dataset.zip\"\r\n",
        "    wget \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\" -O \"data/flickr8k/Flickr8k_text.zip\"\r\n",
        "fi\r\n",
        "\r\n",
        "if [ ! -d \"data/flickr8k/Flicker8k_Dataset\" ]\r\n",
        "then\r\n",
        "    unzip \"data/flickr8k/Flickr8k_Dataset.zip\" -d data/flickr8k/\r\n",
        "fi\r\n",
        "\r\n",
        "if [ ! -d \"data/flickr8k/Flickr8k_text\" ]\r\n",
        "then\r\n",
        "    unzip \"data/flickr8k/Flickr8k_text.zip\" -d data/flickr8k/Flickr8k_text\r\n",
        "    rm -r \"data/flickr8k/Flickr8k_text/__MACOSX\"\r\n",
        "fi\r\n",
        "\r\n",
        "if [ -d \"data/flickr8k/__MACOSX\" ]\r\n",
        "then\r\n",
        "    rm -r \"data/flickr8k/__MACOSX\"\r\n",
        "fi\r\n",
        "mkdir -p saved_models\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdeQH6BU2iu8"
      },
      "source": [
        "%%shell\r\n",
        "\r\n",
        "if [ ! -f \"data/glove.6B.zip\" ]\r\n",
        "then\r\n",
        "    wget \"http://nlp.stanford.edu/data/glove.6B.zip\" -O \"data/glove.6B.zip\"\r\n",
        "fi\r\n",
        "\r\n",
        "if [ ! -d \"data/glove.6B\" ]\r\n",
        "then\r\n",
        "    unzip \"data/glove.6B.zip\" -d \"data/glove.6B\"\r\n",
        "fi\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azA-pSDp2uAo"
      },
      "source": [
        "###Setup Depencencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_zVnDie8rVn"
      },
      "source": [
        "%%capture\r\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO9F2U7M2sJy"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u89SlzKS1vUZ"
      },
      "source": [
        "#Demo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "quhow-bR1lse"
      },
      "source": [
        "MODEL_DIR = 'saved_models/resnet101_attention_frozenENC_trainEMBD_rmsprop_b128_emdGLV300/'\n",
        "MODEL_NAME = 'resnet101_attention_frozenENC_trainEMBD_rmsprop_b128_emdGLV300_best_train.pt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Gi33_M6R1lsj"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from datasets.flickr8k import Flickr8kDataset\n",
        "from metrics import *\n",
        "from utils_torch import *\n",
        "from utils_plot import visualize_att"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "yUGsUwq01lsm"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9x9Ro8oj1lso"
      },
      "source": [
        "DATASET_BASE_PATH = 'data/flickr8k/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YK01t4tF1lsq"
      },
      "source": [
        "vocab_set = pickle.load(open(MODEL_DIR+'vocab_set.pkl', 'rb')) if os.path.exists(MODEL_DIR+'vocab_set.pkl') else None\n",
        "vocab, word2idx, idx2word, max_len = vocab_set\n",
        "vocab_size = len(vocab)\n",
        "vocab_size, max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gmJUZkKe1lst"
      },
      "source": [
        "val_set = Flickr8kDataset(dataset_base_path=DATASET_BASE_PATH, dist='val', vocab_set=vocab_set, device=device,\n",
        "                          return_type='corpus',\n",
        "                          load_img_to_memory=False)\n",
        "test_set = Flickr8kDataset(dataset_base_path=DATASET_BASE_PATH, dist='test', vocab_set=vocab_set, device=device,\n",
        "                           return_type='corpus',\n",
        "                           load_img_to_memory=False)\n",
        "train_eval_set = Flickr8kDataset(dataset_base_path=DATASET_BASE_PATH, dist='train', vocab_set=vocab_set, device=device,\n",
        "                                 return_type='corpus',\n",
        "                                 load_img_to_memory=False)\n",
        "len(train_eval_set), len(val_set), len(test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1kpThDSy1lsv"
      },
      "source": [
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_SIZE = 256\n",
        "BATCH_SIZE = 16\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YxBtlYuy1lsy"
      },
      "source": [
        "checkpoint = torch.load(os.path.join(MODEL_DIR, MODEL_NAME))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bSvbQub81ls1"
      },
      "source": [
        "def evaluate_model(data_loader, model, loss_fn, vocab_size, bleu_score_fn, tensor_to_word_fn, desc=''):\n",
        "    running_bleu = [0.0] * 5\n",
        "    model.eval()\n",
        "    t = tqdm(iter(data_loader), desc=f'{desc}')\n",
        "    for batch_idx, batch in enumerate(t):\n",
        "        images, captions, lengths = batch\n",
        "        outputs = tensor_to_word_fn(model.sample(images).cpu().numpy())\n",
        "\n",
        "        for i in (1, 2, 3, 4):\n",
        "            running_bleu[i] += bleu_score_fn(reference_corpus=captions, candidate_corpus=outputs, n=i)\n",
        "        t.set_postfix({\n",
        "            'bleu1': running_bleu[1] / (batch_idx + 1),\n",
        "            'bleu4': running_bleu[4] / (batch_idx + 1),\n",
        "        }, refresh=True)\n",
        "    for i in (1, 2, 3, 4):\n",
        "        running_bleu[i] /= len(data_loader)\n",
        "    return running_bleu\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "W9SlKOz01ls4"
      },
      "source": [
        "from models.torch.vgg16_monolstm import Captioner\n",
        "\n",
        "final_model = Captioner(EMBEDDING_DIM, HIDDEN_SIZE, vocab_size, num_layers=2).to(device)\n",
        "final_model.load_state_dict(checkpoint['state_dict'])\n",
        "final_model.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "w7OCgym81ls7"
      },
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=train_eval_set.pad_value).to(device)\n",
        "sentence_bleu_score_fn = bleu_score_fn(4, 'sentence')\n",
        "corpus_bleu_score_fn = bleu_score_fn(4, 'corpus')\n",
        "tensor_to_word_fn = words_from_tensors_fn(idx2word=idx2word)\n",
        "\n",
        "eval_transformations = transforms.Compose([\n",
        "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
        "    transforms.CenterCrop(256),  # get 256x256 crop from random location\n",
        "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
        "                         (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "val_set.transformations = eval_transformations\n",
        "test_set.transformations = eval_transformations\n",
        "train_eval_set.transformations = eval_transformations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PH-UxtRT1ls-"
      },
      "source": [
        "eval_collate_fn = lambda batch: (torch.stack([x[0] for x in batch]), [x[1] for x in batch], [x[2] for x in batch])\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
        "                        collate_fn=eval_collate_fn)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
        "                         collate_fn=eval_collate_fn)\n",
        "train_eval_loader = DataLoader(train_eval_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
        "                               collate_fn=eval_collate_fn)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QtFpqbr_1ltA"
      },
      "source": [
        "model = final_model\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bo5tK0IY1ltB"
      },
      "source": [
        "t_i = 100\n",
        "dset = train_eval_set\n",
        "im, cp, _ = dset[t_i]\n",
        "capidx = model.sample(im.unsqueeze(0))[0].detach().cpu().numpy()\n",
        "print(dset.get_image_captions(t_i)[1])\n",
        "caption_pred=''.join(list(itertools.takewhile(lambda word: word.strip() != '<end>',\n",
        "                                                         map(lambda idx: idx2word[idx]+' ', iter(capidx))))[1:])\n",
        "print(f'greedy', caption_pred)\n",
        "for k in (3,5,7):\n",
        "  capidx = model.sample_beam_search(im.unsqueeze(0), beam_width=k)[0]\n",
        "  caption_pred=''.join(list(itertools.takewhile(lambda word: word.strip() != '<end>',\n",
        "                                                          map(lambda idx: idx2word[idx]+' ', iter(capidx))))[1:])\n",
        "  print(f'beam_width={k}', caption_pred)\n",
        "Image.open(dset.get_image_captions(t_i)[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "zzTD08d81ltE"
      },
      "source": [
        "t_i = 500\n",
        "dset = val_set\n",
        "im, cp, _ = dset[t_i]\n",
        "capidx = model.sample(im.unsqueeze(0))[0].detach().cpu().numpy()\n",
        "print(dset.get_image_captions(t_i)[1])\n",
        "caption_pred=''.join(list(itertools.takewhile(lambda word: word.strip() != '<end>',\n",
        "                                                         map(lambda idx: idx2word[idx]+' ', iter(capidx))))[1:])\n",
        "print(f'greedy', caption_pred)\n",
        "for k in (3,5,7):\n",
        "  capidx = model.sample_beam_search(im.unsqueeze(0), beam_width=k)[0]\n",
        "  caption_pred=''.join(list(itertools.takewhile(lambda word: word.strip() != '<end>',\n",
        "                                                          map(lambda idx: idx2word[idx]+' ', iter(capidx))))[1:])\n",
        "  print(f'beam_width={k}', caption_pred)\n",
        "Image.open(dset.get_image_captions(t_i)[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bqowdyT21ltG"
      },
      "source": [
        "t_i = 500\n",
        "dset = test_set\n",
        "im, cp, _ = dset[t_i]\n",
        "capidx = model.sample(im.unsqueeze(0))[0].detach().cpu().numpy()\n",
        "print(dset.get_image_captions(t_i)[1])\n",
        "caption_pred=''.join(list(itertools.takewhile(lambda word: word.strip() != '<end>',\n",
        "                                                         map(lambda idx: idx2word[idx]+' ', iter(capidx))))[1:])\n",
        "print(f'greedy', caption_pred)\n",
        "for k in (3,5,7):\n",
        "  capidx = model.sample_beam_search(im.unsqueeze(0), beam_width=k)[0]\n",
        "  caption_pred=''.join(list(itertools.takewhile(lambda word: word.strip() != '<end>',\n",
        "                                                          map(lambda idx: idx2word[idx]+' ', iter(capidx))))[1:])\n",
        "  print(f'beam_width={k}', caption_pred)\n",
        "Image.open(dset.get_image_captions(t_i)[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JGqLKiYJ1ltH"
      },
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    train_bleu = evaluate_model(desc=f'Train: ', model=final_model,\n",
        "                                loss_fn=loss_fn, bleu_score_fn=corpus_bleu_score_fn,\n",
        "                                tensor_to_word_fn=tensor_to_word_fn,\n",
        "                                data_loader=train_eval_loader, vocab_size=vocab_size)\n",
        "    val_bleu = evaluate_model(desc=f'Val: ', model=final_model,\n",
        "                              loss_fn=loss_fn, bleu_score_fn=corpus_bleu_score_fn,\n",
        "                              tensor_to_word_fn=tensor_to_word_fn,\n",
        "                              data_loader=val_loader, vocab_size=vocab_size)\n",
        "    test_bleu = evaluate_model(desc=f'Test: ', model=final_model,\n",
        "                               loss_fn=loss_fn, bleu_score_fn=corpus_bleu_score_fn,\n",
        "                               tensor_to_word_fn=tensor_to_word_fn,\n",
        "                               data_loader=test_loader, vocab_size=vocab_size)\n",
        "    for setname, result in zip(('train', 'val', 'test'), (train_bleu, val_bleu, test_bleu)):\n",
        "        print(setname, end=' ')\n",
        "        for ngram in (1, 2, 3, 4):\n",
        "            print(f'Bleu-{ngram}: {result[ngram]}', end=' ')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}